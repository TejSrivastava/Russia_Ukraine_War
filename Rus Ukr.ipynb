{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dce5f7c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wordcloud\n",
      "  Downloading wordcloud-1.8.2.2-cp39-cp39-win_amd64.whl (153 kB)\n",
      "     -------------------------------------- 153.1/153.1 kB 1.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: matplotlib in c:\\users\\tejas\\anaconda3\\lib\\site-packages (from wordcloud) (3.5.2)\n",
      "Requirement already satisfied: numpy>=1.6.1 in c:\\users\\tejas\\anaconda3\\lib\\site-packages (from wordcloud) (1.21.5)\n",
      "Requirement already satisfied: pillow in c:\\users\\tejas\\anaconda3\\lib\\site-packages (from wordcloud) (9.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\tejas\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (2.8.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\tejas\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (4.25.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\tejas\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (21.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\tejas\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (1.4.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\tejas\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (3.0.9)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\tejas\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (0.11.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\tejas\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.16.0)\n",
      "Installing collected packages: wordcloud\n",
      "Successfully installed wordcloud-1.8.2.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6c8a441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    id      conversation_id               created_at  \\\n",
      "0  1630366235354451969  1630152070530576385  2023-02-28 00:36:15 UTC   \n",
      "1  1630366226424778753  1630366226424778753  2023-02-28 00:36:13 UTC   \n",
      "2  1630366225930027011  1630366225930027011  2023-02-28 00:36:13 UTC   \n",
      "3  1630366223056662530  1630351686974992385  2023-02-28 00:36:12 UTC   \n",
      "4  1630366221483884545  1629903982255644672  2023-02-28 00:36:12 UTC   \n",
      "\n",
      "         date      time  timezone              user_id     username  \\\n",
      "0  2023-02-28  00:36:15         0  1493761817406894086  tomasliptai   \n",
      "1  2023-02-28  00:36:13         0  1526694166662721536  paperfloure   \n",
      "2  2023-02-28  00:36:13         0  1053018392939167746    katetbar1   \n",
      "3  2023-02-28  00:36:12         0            602371247    jlhrdhmom   \n",
      "4  2023-02-28  00:36:12         0  1053594763214184448    phemikali   \n",
      "\n",
      "                  name place  ... geo source user_rt_id user_rt retweet_id  \\\n",
      "0         Tomas Liptai   NaN  ... NaN    NaN        NaN     NaN        NaN   \n",
      "1      Smell the roses   NaN  ... NaN    NaN        NaN     NaN        NaN   \n",
      "2                @etak   NaN  ... NaN    NaN        NaN     NaN        NaN   \n",
      "3               JLHrdh   NaN  ... NaN    NaN        NaN     NaN        NaN   \n",
      "4  rolarkcybersecurity   NaN  ... NaN    NaN        NaN     NaN        NaN   \n",
      "\n",
      "                                            reply_to  retweet_date  translate  \\\n",
      "0  [{'screen_name': 'nazijaeger__', 'name': 'nazi...           NaN        NaN   \n",
      "1                                                 []           NaN        NaN   \n",
      "2                                                 []           NaN        NaN   \n",
      "3  [{'screen_name': 'MainelifeR', 'name': 'Mainel...           NaN        NaN   \n",
      "4  [{'screen_name': 'Pottingpinks', 'name': 'GRS'...           NaN        NaN   \n",
      "\n",
      "  trans_src trans_dest  \n",
      "0       NaN        NaN  \n",
      "1       NaN        NaN  \n",
      "2       NaN        NaN  \n",
      "3       NaN        NaN  \n",
      "4       NaN        NaN  \n",
      "\n",
      "[5 rows x 36 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "data = pd.read_csv(\"C:\\\\Users\\\\Tejas\\\\Downloads\\\\Russia_Vs_Ukraine_War\\\\filename.csv\")\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35efcd65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'conversation_id', 'created_at', 'date', 'time', 'timezone',\n",
      "       'user_id', 'username', 'name', 'place', 'tweet', 'language', 'mentions',\n",
      "       'urls', 'photos', 'replies_count', 'retweets_count', 'likes_count',\n",
      "       'hashtags', 'cashtags', 'link', 'retweet', 'quote_url', 'video',\n",
      "       'thumbnail', 'near', 'geo', 'source', 'user_rt_id', 'user_rt',\n",
      "       'retweet_id', 'reply_to', 'retweet_date', 'translate', 'trans_src',\n",
      "       'trans_dest'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e7f8717",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[[\"username\", \"tweet\", \"language\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67f877a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "username    0\n",
       "tweet       0\n",
       "language    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "605abf9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "en     8858\n",
       "pt      440\n",
       "it      194\n",
       "qme     105\n",
       "und      60\n",
       "in       47\n",
       "ru       44\n",
       "ja       42\n",
       "es       36\n",
       "ca       20\n",
       "qht      20\n",
       "th       19\n",
       "fr       18\n",
       "de       14\n",
       "ko        9\n",
       "vi        8\n",
       "nl        8\n",
       "ro        7\n",
       "fi        7\n",
       "ar        6\n",
       "zxx       6\n",
       "uk        6\n",
       "cs        6\n",
       "zh        5\n",
       "pl        5\n",
       "qam       4\n",
       "tl        4\n",
       "da        3\n",
       "eu        2\n",
       "no        2\n",
       "hi        2\n",
       "tr        2\n",
       "hu        1\n",
       "cy        1\n",
       "lv        1\n",
       "el        1\n",
       "bn        1\n",
       "Name: language, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"language\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f962650e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Tejas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stemmer = nltk.SnowballStemmer(\"english\")\n",
    "stopword=set(stopwords.words('english'))\n",
    "\n",
    "def clean(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    text = [word for word in text.split(' ') if word not in stopword]\n",
    "    text=\" \".join(text)\n",
    "    text = [stemmer.stem(word) for word in text.split(' ')]\n",
    "    text=\" \".join(text)\n",
    "    return text\n",
    "data[\"tweet\"] = data[\"tweet\"].apply(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c923ba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Tejas\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               tweet  Positive  Negative  \\\n",
      "0      nazijaeg derwen  russia place satan rule well     0.259     0.000   \n",
      "1  russia haarp could destroy usa one fell swoop ...     0.000     0.280   \n",
      "2       putin give steven seagal  order friendship       0.367     0.000   \n",
      "3          mainelif baddcompani alway project russia     0.000     0.000   \n",
      "4  pottingpink mfarussia modrussia milhistrf muze...     0.068     0.078   \n",
      "\n",
      "   Neutral  \n",
      "0    0.741  \n",
      "1    0.720  \n",
      "2    0.633  \n",
      "3    1.000  \n",
      "4    0.854  \n"
     ]
    }
   ],
   "source": [
    "nltk.download('vader_lexicon')\n",
    "sentiments = SentimentIntensityAnalyzer()\n",
    "data[\"Positive\"] = [sentiments.polarity_scores(i)[\"pos\"] for i in data[\"tweet\"]]\n",
    "data[\"Negative\"] = [sentiments.polarity_scores(i)[\"neg\"] for i in data[\"tweet\"]]\n",
    "data[\"Neutral\"] = [sentiments.polarity_scores(i)[\"neu\"] for i in data[\"tweet\"]]\n",
    "data = data[[\"tweet\", \"Positive\", \"Negative\", \"Neutral\"]]\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1a053a",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative =' '.join([i for i in data['tweet'][data['Negative'] > data[\"Positive\"]]])\n",
    "stopwords = set(STOPWORDS)\n",
    "wordcloud = WordCloud(stopwords=stopwords, background_color=\"white\").generate(negative)\n",
    "plt.figure( figsize=(15,10))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
